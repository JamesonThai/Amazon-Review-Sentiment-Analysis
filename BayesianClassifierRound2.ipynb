{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import pickle\n",
    "# from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Models With PICKLE\n",
    "with open(\"classifierHalf.pkl\", 'rb') as file:  \n",
    "    classifierPickleHalf = pickle.load(file)\n",
    "    \n",
    "with open(\"classifierNormal.pkl\", 'rb') as file:  \n",
    "    classifierNormal = pickle.load(file) \n",
    "\n",
    "with open(\"classifierSmall.pkl\", 'rb') as file:  \n",
    "    classifierSmall = pickle.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in All data for all parsed/cleaned ratings\n",
    "dataTestR1 = pd.read_csv('parsed_data/testing_data/reviews1.txt', sep=\"\\n\", header=None)\n",
    "dataTestR1.columns = [\"Review\"]\n",
    "dataTestR1.loc[:,'Rating'] = 1\n",
    "dataTestR2 = pd.read_csv('parsed_data/testing_data/reviews2.txt', sep=\"\\n\", header=None)\n",
    "dataTestR2.columns = [\"Review\"]\n",
    "dataTestR2.loc[:,'Rating'] = 2\n",
    "dataTestR3 = pd.read_csv('parsed_data/testing_data/reviews3.txt', sep=\"\\n\", header=None)\n",
    "dataTestR3.columns = [\"Review\"]\n",
    "dataTestR3.loc[:,'Rating'] = 3\n",
    "dataTestR4 = pd.read_csv('parsed_data/testing_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTestR4.columns = [\"Review\"]\n",
    "dataTestR4.loc[:,'Rating'] = 4\n",
    "dataTestR5 = pd.read_csv('parsed_data/testing_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTestR5.columns = [\"Review\"]\n",
    "dataTestR5.loc[:,'Rating'] = 5\n",
    "\n",
    "dataTrainR1 = pd.read_csv('parsed_data/training_data/reviews1.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR1.columns = [\"Review\"]\n",
    "dataTrainR1.loc[:,'Rating'] = 1\n",
    "dataTrainR2 = pd.read_csv('parsed_data/training_data/reviews2.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR2.columns = [\"Review\"]\n",
    "dataTrainR2.loc[:,'Rating'] = 2\n",
    "dataTrainR3 = pd.read_csv('parsed_data/training_data/reviews3.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR3.columns = [\"Review\"]\n",
    "dataTrainR3.loc[:,'Rating'] = 3\n",
    "dataTrainR4 = pd.read_csv('parsed_data/training_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR4.columns = [\"Review\"]\n",
    "dataTrainR4.loc[:,'Rating'] = 4\n",
    "dataTrainR5 = pd.read_csv('parsed_data/training_data/reviews5.txt', sep=\"\\n\", header=None)\n",
    "dataTrainR5.columns = [\"Review\"]\n",
    "dataTrainR5.loc[:,'Rating'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as an avid reader i was not happy with this bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ever since the action in the anita blake shift...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when historians go dumpster diving through the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im a big fan of michael crichton and the huge ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i might have considered buying this ebook if t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  as an avid reader i was not happy with this bo...       1\n",
       "1  ever since the action in the anita blake shift...       1\n",
       "2  when historians go dumpster diving through the...       1\n",
       "3  im a big fan of michael crichton and the huge ...       1\n",
       "4  i might have considered buying this ebook if t...       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AggregatingMore into each test Full 80 to 20% for random values\n",
    "framesTrainFull = [dataTrainR1,dataTrainR2,dataTrainR3,dataTrainR4,dataTrainR5]\n",
    "framesTestFull = [dataTestR1, dataTestR2, dataTestR3 , dataTestR4 , dataTestR5]\n",
    "trainingFull = pd.concat(framesTrainFull)\n",
    "testingFull = pd.concat(framesTestFull)\n",
    "\n",
    "# test Half\n",
    "framesTrainHalf = [dataTrainR1.sample(n=25000),dataTrainR2.sample(n=25000),dataTrainR3.sample(n=25000),dataTrainR4.sample(25000),dataTrainR5.sample(n=25000)]\n",
    "framesTestHalf = [dataTestR1.sample(10000), dataTestR2.sample(10000), dataTestR3.sample(10000) , dataTestR4.sample(10000) , dataTestR5.sample(10000)]\n",
    "trainingHalf = pd.concat(framesTrainHalf)\n",
    "testingHalf = pd.concat(framesTestHalf)\n",
    "# Test Small\n",
    "framesTrainSmall = [dataTrainR1.sample(10000),dataTrainR2.sample(10000),dataTrainR3.sample(10000),dataTrainR4.sample(10000),dataTrainR5.sample(10000)]\n",
    "framesTestSmall = [dataTestR1.sample(2500), dataTestR2.sample(2500), dataTestR3.sample(2500) , dataTestR4.sample(2500) , dataTestR5.sample(2500)]\n",
    "trainingSmall = pd.concat(framesTrainSmall)\n",
    "testingSmall = pd.concat(framesTestSmall)\n",
    "trainingFull.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22031</th>\n",
       "      <td>i understand why newman cringed over this film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31175</th>\n",
       "      <td>how can anyone keep reading these novels is be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27325</th>\n",
       "      <td>this book was a romance novel with little to d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>this season was made by the usa network they w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31699</th>\n",
       "      <td>not my cup of tea  i was really hoping to like...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  Rating\n",
       "22031  i understand why newman cringed over this film...       1\n",
       "31175  how can anyone keep reading these novels is be...       1\n",
       "27325  this book was a romance novel with little to d...       1\n",
       "3007   this season was made by the usa network they w...       1\n",
       "31699  not my cup of tea  i was really hoping to like...       1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingHalf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# framesTrainHalf = [dataTrainR1[:10000],dataTrainR2[:10000],dataTrainR3[:10000],dataTrainR4[:10000],dataTrainR5[:10000]]\n",
    "# trainingHalf = pd.concat(framesTrainHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_in_reviews(reviews):\n",
    "    all_words = []\n",
    "    for sentence, rating in zip(training.iloc[:,0], training.iloc[:,1]):\n",
    "        print(sentence, rating)\n",
    "        sentence = str(sentence)\n",
    "        all_words.extend(sentence)\n",
    "    return all_words\n",
    "def get_rating_features(reviewList):\n",
    "    reviewList = nltk.FreqDist(reviewList)\n",
    "    feature = reviewList.keys()\n",
    "    return feature\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' %word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Going Through All DataSets\n",
    "for index, row in trainingFull.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        trainingFull.drop(index, inplace=True)\n",
    "for index, row in trainingSmall.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        trainingSmall.drop(index, inplace=True)\n",
    "for index, row in trainingHalf.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        trainingHalf.drop(index, inplace=True)\n",
    "        \n",
    "# Going through TrainR1\n",
    "for index, row in dataTrainR1.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR1.drop(index, inplace=True)\n",
    "# Going Through TrainR2\n",
    "for index, row in dataTrainR2.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR2.drop(index, inplace=True)\n",
    "# Going Through TrainR3\n",
    "for index, row in dataTrainR3.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR3.drop(index, inplace=True)\n",
    "# Going Through TrainR4\n",
    "for index, row in dataTrainR4.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR4.drop(index, inplace=True)\n",
    "# Going Through Train R5\n",
    "for index, row in dataTrainR5.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        dataTrainR5.drop(index, inplace=True)\n",
    "        \n",
    "# Going Through TestingSet\n",
    "for index, row in testingFull.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        testingFull.drop(index, inplace=True)\n",
    "for index, row in testingSmall.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        testingSmall.drop(index, inplace=True)\n",
    "for index, row in testingHalf.iterrows():\n",
    "    if type(row['Review']) is float:\n",
    "        testingHalf.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    avid reader happy book seemed rushed character...\n",
       "1    ever since action anita blake shifted action s...\n",
       "2    historians go dumpster diving cultural effluvi...\n",
       "3    im big fan michael crichton huge controversy l...\n",
       "4    might considered buying ebook look inside show...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Stopwords, rare words from datasets\n",
    "stop = stopwords.words('english')\n",
    "trainingFull['Review'] = trainingFull['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "testingFull['Review'] = testingFull['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "trainingFull['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as above but just for the rest\n",
    "trainingHalf['Review'] = trainingHalf['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "testingHalf['Review'] = testingHalf['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "trainingSmall['Review'] = trainingSmall['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "testingSmall['Review'] = testingSmall['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing for the Others\n",
    "dataTrainR1['Review'] = dataTrainR1['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR2['Review'] = dataTrainR2['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR3['Review'] = dataTrainR3['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR4['Review'] = dataTrainR4['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dataTrainR5['Review'] = dataTrainR5['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book          60398\n",
       "one           30431\n",
       "like          24052\n",
       "read          23811\n",
       "story         21937\n",
       "would         19980\n",
       "good          17107\n",
       "really        15521\n",
       "get           14447\n",
       "much          14301\n",
       "time          14265\n",
       "even          13195\n",
       "first         12639\n",
       "great         12493\n",
       "characters    12217\n",
       "well          11944\n",
       "love          11836\n",
       "books         11481\n",
       "dont          10943\n",
       "movie         10785\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is the generally the most frequent world cap\n",
    "# This is further finetune for removing common words\n",
    "x = 20\n",
    "# For Small\n",
    "freqTotalSmall = pd.Series(' '.join(trainingSmall['Review']).split()).value_counts()[:x]\n",
    "freqTestingTotalSmall = pd.Series(' '.join(testingSmall['Review']).split()).value_counts()[:x]\n",
    "freqTotalSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book          150769\n",
       "one            75466\n",
       "like           60650\n",
       "read           59875\n",
       "story          54444\n",
       "would          49861\n",
       "good           42650\n",
       "really         38516\n",
       "get            36377\n",
       "time           35864\n",
       "much           35537\n",
       "even           32778\n",
       "first          31334\n",
       "great          31085\n",
       "characters     30682\n",
       "love           29881\n",
       "well           29676\n",
       "books          28935\n",
       "dont           27685\n",
       "movie          27621\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is the generally the most frequent world cap\n",
    "# This is further finetune for removing common words\n",
    "# For Half\n",
    "freqTotalHalf = pd.Series(' '.join(trainingHalf['Review']).split()).value_counts()[:x]\n",
    "freqTestingTotalHalf = pd.Series(' '.join(testingHalf['Review']).split()).value_counts()[:x]\n",
    "freqTotalHalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book          302098\n",
       "one           152305\n",
       "like          121311\n",
       "read          119507\n",
       "story         109497\n",
       "would          99940\n",
       "good           85693\n",
       "really         77983\n",
       "get            72910\n",
       "time           71818\n",
       "much           71251\n",
       "even           66033\n",
       "first          63016\n",
       "characters     62089\n",
       "great          61991\n",
       "love           59676\n",
       "well           59244\n",
       "books          57676\n",
       "dont           55605\n",
       "movie          55050\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X is the generally the most frequent world cap\n",
    "# This is further finetune for removing common words\n",
    "# For Full\n",
    "freqTotalFull = pd.Series(' '.join(trainingFull['Review']).split()).value_counts()[:x]\n",
    "freqTestingTotalFull = pd.Series(' '.join(testingFull['Review']).split()).value_counts()[:x]\n",
    "freqTotalFull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rare Words Removal\n",
    "Y = -100\n",
    "rareWordsTrainingFull = pd.Series(' '.join(trainingFull['Review']).split()).value_counts()[Y:]\n",
    "rareWordsTestingFull = pd.Series(' '.join(testingFull['Review']).split()).value_counts()[Y:]\n",
    "# rareWordsTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prep to remove RARE and fairly common Words Method 1, THE FOLLOWING is for FULL, you can change the names to small or half\n",
    "freqTotalFull = list(freqTotalFull.index)\n",
    "freqTestingTotalFull = list(freqTestingTotalFull.index)\n",
    "rareWordsTrainingFull = list(rareWordsTestingFull.index)\n",
    "rareWordsTestingFull = list(rareWordsTestingFull.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For convienence \n",
    "trainingFinal = trainingHalf\n",
    "testingFinal = trainingHalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22031    understand newman cringed film pretend cult cl...\n",
       "31175    anyone keep reading novels beyond understandin...\n",
       "27325    romance novel little end worldzombies top teen...\n",
       "3007     season made usa network unable procure airwolf...\n",
       "31699    cup tea hoping could find poem enjoyed hoping ...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingFinal['Review'] = trainingFinal['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqTotalFull))\n",
    "trainingFinal['Review'] = trainingFinal['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareWordsTrainingFull))\n",
    "trainingFinal['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22031    understand newman cringed film pretend cult cl...\n",
       "31175    anyone keep reading novels beyond understandin...\n",
       "27325    romance novel little end worldzombies top teen...\n",
       "3007     season made usa network unable procure airwolf...\n",
       "31699    cup tea hoping could find poem enjoyed hoping ...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testingFinal['Review'] = testingFinal['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqTestingTotalFull))\n",
    "testingFinal['Review'] = testingFinal['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareWordsTestingFull))\n",
    "testingFinal['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('also', 26476),\n",
       " ('could', 26137),\n",
       " ('way', 25552),\n",
       " ('many', 24822),\n",
       " ('people', 22457),\n",
       " ('know', 22074),\n",
       " ('reading', 22072),\n",
       " ('author', 21843),\n",
       " ('think', 21504),\n",
       " ('life', 21019),\n",
       " ('two', 20793),\n",
       " ('see', 20759),\n",
       " ('little', 20485),\n",
       " ('make', 20307),\n",
       " ('didnt', 20291),\n",
       " ('series', 20270),\n",
       " ('new', 19405),\n",
       " ('im', 19366),\n",
       " ('never', 19340),\n",
       " ('better', 18063),\n",
       " ('film', 17412),\n",
       " ('back', 17227),\n",
       " ('find', 17193),\n",
       " ('work', 17055),\n",
       " ('want', 16909),\n",
       " ('character', 16578),\n",
       " ('end', 16426),\n",
       " ('made', 15940),\n",
       " ('still', 15709),\n",
       " ('found', 15456),\n",
       " ('say', 15227),\n",
       " ('going', 15219),\n",
       " ('another', 14669),\n",
       " ('something', 14648),\n",
       " ('bad', 14321),\n",
       " ('go', 14172),\n",
       " ('things', 14062),\n",
       " ('cant', 13978),\n",
       " ('got', 13966),\n",
       " ('best', 13943),\n",
       " ('doesnt', 13806),\n",
       " ('use', 13766),\n",
       " ('plot', 13576),\n",
       " ('world', 13395),\n",
       " ('lot', 13380),\n",
       " ('us', 13128),\n",
       " ('years', 13091),\n",
       " ('novel', 13054),\n",
       " ('writing', 12998),\n",
       " ('interesting', 12984)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Classifier Part/Bag O WORDS\n",
    "# Only get the Reviews and tokenize\n",
    "TR = trainingFinal.to_csv(header=None, index = False)\n",
    "tokenizedList = nltk.word_tokenize(TR)\n",
    "common = nltk.FreqDist(tokenizedList)\n",
    "# 50 most common in this total distribution\n",
    "common.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Frequent Words for Other Ratings\n",
    "freqR1 = pd.Series(' '.join(dataTrainR1['Review']).split()).value_counts()[:x]\n",
    "freqR2 = pd.Series(' '.join(dataTrainR2['Review']).split()).value_counts()[:x]\n",
    "freqR3 = pd.Series(' '.join(dataTrainR3['Review']).split()).value_counts()[:x]\n",
    "freqR4 = pd.Series(' '.join(dataTrainR4['Review']).split()).value_counts()[:x]\n",
    "freqR5 = pd.Series(' '.join(dataTrainR5['Review']).split()).value_counts()[:x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creation of Bag of words for those classifiers\n",
    "TR1 = dataTrainR1.to_csv(header=None, index = False)\n",
    "tokenizedListR1 = nltk.word_tokenize(TR1)\n",
    "commonR1 = nltk.FreqDist(tokenizedListR1)\n",
    "\n",
    "TR2 = dataTrainR2.to_csv(header=None, index = False)\n",
    "tokenizedListR2 = nltk.word_tokenize(TR2)\n",
    "commonR2 = nltk.FreqDist(tokenizedListR2)\n",
    "\n",
    "TR3 = dataTrainR3.to_csv(header=None, index = False)\n",
    "tokenizedListR3 = nltk.word_tokenize(TR3)\n",
    "commonR3 = nltk.FreqDist(tokenizedListR3)\n",
    "\n",
    "TR4 = dataTrainR4.to_csv(header=None, index = False)\n",
    "tokenizedListR4 = nltk.word_tokenize(TR4)\n",
    "commonR4 = nltk.FreqDist(tokenizedListR4)\n",
    "\n",
    "TR5 = dataTrainR5.to_csv(header=None, index = False)\n",
    "tokenizedListR5 = nltk.word_tokenize(TR5)\n",
    "commonR5 = nltk.FreqDist(tokenizedListR5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This number is just taken randomly for no apparent reason do for all lists then combine them for getting the most common words\n",
    "WordCnt = 2500\n",
    "mostCommonR1 = commonR1.most_common(WordCnt)\n",
    "tempR1 = [x[0] for x in mostCommonR1]\n",
    "mostCommonWordsR1 = set(tempR1)\n",
    "\n",
    "mostCommonR2 = commonR2.most_common(WordCnt)\n",
    "tempR2 = [x[0] for x in mostCommonR2]\n",
    "mostCommonWordsR2 = set(tempR2)\n",
    "\n",
    "mostCommonR3 = commonR3.most_common(WordCnt)\n",
    "tempR3 = [x[0] for x in mostCommonR3]\n",
    "mostCommonWordsR3 = set(tempR3)\n",
    "\n",
    "mostCommonR4 = commonR4.most_common(WordCnt)\n",
    "tempR4 = [x[0] for x in mostCommonR4]\n",
    "mostCommonWordsR4 = set(tempR4)\n",
    "\n",
    "mostCommonR5 = commonR5.most_common(WordCnt)\n",
    "tempR5 = [x[0] for x in mostCommonR5]\n",
    "mostCommonWordsR5 = set(tempR5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Multiple Bag of Words Sets into One\n",
    "mostCommonWordsTotal = mostCommonWordsR1\n",
    "mostCommonWordsTotal.update(mostCommonWordsR2)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR3)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR4)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR5)\n",
    "len(mostCommonWordsTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Weights and see if sentences for those reviews have bag O words present in them\n",
    "import datetime\n",
    "# Just to See When Finished\n",
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT)) \n",
    "ResultTUPOverall = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in trainingFinal.iterrows()]\n",
    "print (str(currentDT)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifierOverall = nltk.NaiveBayesClassifier.train(ResultTUPOverall)\n",
    "classifierOverall.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comparing with Test data \n",
    "ResultTestList = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in testingFinal.iterrows()]\n",
    "# after this feed into accruacy classifier\n",
    "print(nltk.classify.accuracy(classifierOverall, ResultTestList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving Models\n",
    "import pickle\n",
    "\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"classifierSmall.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(classifierSmall, file)\n",
    "\n",
    "pkl_filename = \"classifierOverall.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(classifierOverall, file)\n",
    "\n",
    "pkl_filename = \"classifierHalf.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(classifierHalf, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining Multiple Bag of Words Sets into One\n",
    "mostCommonWordsTotal = mostCommonWordsR1\n",
    "mostCommonWordsTotal.update(mostCommonWordsR2)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR3)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR4)\n",
    "mostCommonWordsTotal.update(mostCommonWordsR5)\n",
    "len(mostCommonWordsTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Half DataSet\n",
    "TRHalf = trainingHalf.to_csv(header=None, index = False)\n",
    "tokenizedListHalf = nltk.word_tokenize(TR)\n",
    "commonHalf = nltk.FreqDist(tokenizedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is for small\n",
    "ResultTupOD = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in training.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   waste = True                1 : 4      =     26.4 : 1.0\n",
      "                  highly = True                5 : 3      =      4.7 : 1.0\n",
      "                   money = True                1 : 4      =      4.5 : 1.0\n",
      "               wonderful = True                5 : 1      =      4.0 : 1.0\n",
      "                   loved = True                5 : 1      =      3.9 : 1.0\n",
      "                 enjoyed = True                3 : 1      =      3.8 : 1.0\n",
      "                     bad = True                1 : 4      =      3.7 : 1.0\n",
      "                   liked = True                3 : 1      =      3.6 : 1.0\n",
      "                   didnt = True                2 : 4      =      3.3 : 1.0\n",
      "                   wasnt = True                2 : 4      =      3.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Classifier SMALL\n",
    "classifierSmall = nltk.NaiveBayesClassifier.train(ResultTupOD)\n",
    "classifierSmall.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what a shame i was really looking forward to a period drama worthy of source material instead i got artsyfartsy rubbish the actors do fine but they seem out of place in this strange conglomeration of stage decorations which only seems to be there for one of two reasons a to show how ohsooriginal the filmmakers are not or b because they didnt want to spend a ton of money on actual realistic scenes of the glorious gorgeous 19th century russia and its high society even though the movie obviously thinks itself ohsooriginal it really isnt how many times have you seen a variation of time stopping as two people are falling in love showing it from a different angle doesnt make it original last but not least anyone care to explain why kremlin which is in moscow is a recurring background for scenes that are supposed to be happening in st petersburg looks like they saved some money on consultants as well so is it epic yes an epic disappointment :  2\n",
      "True:  2  small:  2  half:  2\n"
     ]
    }
   ],
   "source": [
    "# Testing For Small and half\n",
    "# This will randomly select an object from dataset and compare it \n",
    "\n",
    "row = testing.sample(1)\n",
    "test_sentence = row.iloc[0]['Review']\n",
    "rating = row.iloc[0]['Rating']\n",
    "print(test_sentence, \": \", rating)\n",
    "\n",
    "test_sent_features = {word.lower(): (word in nltk.word_tokenize(test_sentence.lower())) for word in mostCommonWordsTotal}\n",
    "resultSmall = classifierSmall.classify(test_sent_features)\n",
    "resultHalf = classifierHalf.classify(test_sent_features)\n",
    "\n",
    "print(\"True: \" , rating , \" small: \", resultSmall, \" half: \", resultHalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(classifierPickleHalf.classify(test_sent_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comparing with Test data \n",
    "ResultTestList = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in testing.iterrows()]\n",
    "# after this feed into accruacy classifier\n",
    "print(nltk.classify.accuracy(classifierHalf, ResultTestList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  This is for Half\n",
    "ResultTupODHalf = [({word: (word in nltk.word_tokenize(row['Review'])) for word in mostCommonWordsTotal}, row['Rating']) for index,row in trainingHalf.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   waste = True                1 : 4      =     27.3 : 1.0\n",
      "                   money = True                1 : 4      =      4.5 : 1.0\n",
      "                  highly = True                5 : 2      =      4.4 : 1.0\n",
      "               wonderful = True                5 : 1      =      4.2 : 1.0\n",
      "                 enjoyed = True                3 : 1      =      3.9 : 1.0\n",
      "                   loved = True                5 : 1      =      3.8 : 1.0\n",
      "                   liked = True                3 : 1      =      3.7 : 1.0\n",
      "                     bad = True                1 : 4      =      3.4 : 1.0\n",
      "                    wait = True                5 : 3      =      3.3 : 1.0\n",
      "                     bit = True                3 : 1      =      3.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Classifier Part\n",
    "classifierHalf = nltk.NaiveBayesClassifier.train(ResultTupODHalf)\n",
    "classifierHalf.show_most_informative_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
